# Quick tour

________________________________________________________

## Get started

To get started, we will show how to consume remotely hosted LLMs with our Zero-trust AI APIs using the BlindLlama Python SDK.

In this example, we will show how you can start experimenting with Llama 2 quickly through our SaaS, without having to worry about exposing your data to us.

### Installation

First, we need to install BlindLlama open-source Python client:

```bash
pip install blind_llama
```

### Get your access token

Then, you will need to provide a token. You can get one on our [Mithril Cloud](https://cloud.mithrilsecurity.io/) to get started for free.

### Run

Now you can start playing with our privacy-friendly Llama 2 model:

```python
import blind_llama

# connect to the BlindLLama server using your API key
client = blind_llama.connect("<YOUR_API_KEY>")

# use predict method to query the Llama2 model with a prompt of your choice
ret = client.predict("Can you explain to me briefly what is Python programming language?")

# print the results
print(ret)
```

### Security

What we showed you may not look different from other AI APIs provider. 

But a lot is happening under the hood with this line to ensure your data will be kept confiential when querying the Llama2 model: 
```python
client = blind_llama.connect("<YOUR_API_KEY>")
```

As we will see in depth in further sections, our Python SDK first attests that we are talking to a server that analyzes data without exposing it to the admins of the service. 

This is done thanks to two things:

+ **Hardened data controls:** remove all ability from the admins to have access to users data by removing all exposure channels, network, logs, and so on.
+ **Integrity of code:** cryptographically prove that the mentioned controls are actually in place with irrefutable proof generated by secure hardware.

We will explain those in the next section on our architecture for a Zero-trust AI service.