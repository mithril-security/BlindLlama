# Quick tour

________________________________________________________

## Get started

To get started, we will show how to consume remotely hosted LLMs with our Zero-trust AI APIs using the BlindLlama Python SDK.

In this example, we will show how you can start experimenting with Llama 2 quickly through our SaaS, without having to worry about exposing your data to us.

### Installation

First, we need to install BlindLlama open-source Python client:

```bash
pip install blind_llama
```

### Authenticate

Then, you will need to provide a token. You can get one on our [Mithril Cloud](https://cloud.mithrilsecurity.io/) to get started for free. It that can then be set in as an environment variable:

```bash
export BLIND_LLAMA_API_KEY=<YOUR_API_KEY>
```

### Run

Now you can start playing with our privacy-friendly Llama 2 model:

```python
import blind_llama

# connect to the BlindLLama server using your API key
client = blind_llama.connect("<YOUR_API_KEY>")

# use predict method to query the Llama2 model with a prompt of your choice
ret = client.predict("Can you explain to me briefly what is Python programming language?")

# print the results
print(ret)
```

### Security

What we showed you most likely did not impress you and what we provide does not look different from other AI APIs provider. 

However a lot happen with this line: 
```python
client = blind_llama.connect("<YOUR_API_KEY>")
```

Indeed, as we will see, our Python SDK first attests that we are talking to a server that necessarily analyzes data without exposing it to the admins of the service. 

This is done thanks to two things:

+ **Hardened data controls:** remove all ability from the admins to have access to users data by removing all exposure channels, network, logs, and so on.
+ **Integrity of code:** cryptographically prove that the mentioned controls are actually in place with irrefutable proof generated by secure hardware.

We will explain those in the next section on our architecture for a Zero-trust AI service.